{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# relu with 1 layer, euclidean distance between each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvidia_smi.nvmlInit()\n",
    "cuda = nvidia_smi.nvmlDeviceGetHandleByIndex(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config('config-Copy13.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Net_NOD_BN_Skip_SuperDeep(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_layers = 22\n",
    "        \n",
    "        out_len = 2\n",
    "        in_vf_lens = [2, 8] + [16] * (self.n_layers - 3) + [8]\n",
    "        out_vf_lens = in_vf_lens[1:] + [out_len]\n",
    "        in_ef_lens = [2] + [3] * (self.n_layers - 1)\n",
    "        layer_bn = [True] * (self.n_layers - 1) + [False]\n",
    "        layer_dp = [0.2] * (self.n_layers - 2) + [None] * 2\n",
    "        \n",
    "        self.skip = {\n",
    "            0: None, \n",
    "            1: None,\n",
    "            2: None, \n",
    "            3: 2, \n",
    "            4: None, \n",
    "            5: 4, \n",
    "            6: None, \n",
    "            7: 6, \n",
    "            8: None, \n",
    "            9: 8, \n",
    "            10: None, \n",
    "            11: 10, \n",
    "            12: None, \n",
    "            13: 12, \n",
    "            14: None, \n",
    "            15: 14, \n",
    "            16: None, \n",
    "            17: 16, \n",
    "            18: None,\n",
    "            19: 18,\n",
    "            20: None,\n",
    "            21: None\n",
    "        }\n",
    "        \n",
    "        self.conv = nn.ModuleList()\n",
    "        self.bn = nn.ModuleList()\n",
    "        self.dp = nn.ModuleList()\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        \n",
    "        for layer in range(self.n_layers):\n",
    "            efeat_net = nn.Sequential(\n",
    "                Linear(in_ef_lens[layer], in_vf_lens[layer] * out_vf_lens[layer]),\n",
    "                nn.Tanh()\n",
    "            )\n",
    "            self.conv.append(NNConv(in_vf_lens[layer], out_vf_lens[layer], efeat_net, aggr='mean'))\n",
    "            self.bn.append(BatchNorm(out_vf_lens[layer]) if layer_bn[layer] else Identity())\n",
    "            self.dp.append(Dropout(layer_dp[layer]) if layer_dp[layer] is not None else Identity())\n",
    "\n",
    "    def _layer(self, idx, data, v_feat, e_feat, skip_conn=None):\n",
    "        vout = self.dp[idx](self.relu(self.bn[idx](self.conv[idx](v_feat, data.edge_index, e_feat))))\n",
    "        vout = vout if skip_conn is None else vout + skip_conn\n",
    "        eout = torch.cat([data.edge_attr, get_euclidian(vout, data)], dim=1)\n",
    "        return vout, eout\n",
    "            \n",
    "    def forward(self, data):\n",
    "        x, e = [], []\n",
    "        \n",
    "        x.append(data.x)\n",
    "        e.append(data.edge_attr)\n",
    "        \n",
    "        for layer in range(self.n_layers):\n",
    "            skip_conn = None if self.skip[layer] is None else x[self.skip[layer]]\n",
    "            x_, e_ = self._layer(layer, data, x[layer], e[layer], skip_conn=skip_conn)\n",
    "            x.append(x_)\n",
    "            e.append(e_)\n",
    "\n",
    "        return x[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_list, data_list = load_processed_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(data_list[:10000], batch_size=config['batchsize'],shuffle=True)\n",
    "loss_ep = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = config['epoch']['start']\n",
    "if start_epoch == 0:\n",
    "    model = Net_NOD_BN_Skip_SuperDeep().to(device)\n",
    "else:\n",
    "    model = torch.load(f\"../ckpt_{config['name']}/epoch_{start_epoch}.pt\").to(device)\n",
    "criterion = EnergyLossVectorized()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr']['initial'])\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optimizer, \n",
    "                                            step_size=config['lr']['decay_step'], \n",
    "                                            gamma=config['lr']['decay_rate'])\n",
    "print(\"=\" * 50, file=open(f\"{config['name']}.log\", \"a\"))\n",
    "epoch = start_epoch + 1\n",
    "with tqdm(total=len(loader), smoothing=0) as progress:\n",
    "    while True:\n",
    "        if config['lr']['override'] is not None:\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr']['override'])\n",
    "            scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optimizer, \n",
    "                                                        step_size=config['lr']['decay_step'], \n",
    "                                                        gamma=config['lr']['decay_rate'])\n",
    "        progress.reset()\n",
    "        progress.set_description(desc=f\"[epoch {epoch}/{config['epoch']['end']}]\")\n",
    "        train_loss = train(model, criterion, optimizer, loader, data_list, device, progress, cuda)\n",
    "        loss_ep.append(train_loss)\n",
    "        scheduler.step()\n",
    "        if epoch == 1 and config['log_period'] != 1:\n",
    "            print(epoch, loss, scheduler.get_lr(), file=open(f\"{config['name']}.log\", \"a\"))\n",
    "        if epoch % config['log_period'] == 0:\n",
    "            torch.save(model, f\"../ckpt_{config['name']}/epoch_{epoch}.pt\")\n",
    "            test_loss = []\n",
    "            for val_idx in range(11000, len(G_list)):\n",
    "                node_pos,loss = evaluate(model, data_list[val_idx],criterion,device)\n",
    "                test_loss.append(loss)\n",
    "            print(f'{epoch}, train: {train_loss}, val:{np.mean(test_loss)}ï¼Œ{scheduler.get_lr()}', \n",
    "                  file=open(f\"{config['name']}.log\", \"a\"))\n",
    "        if epoch == config['epoch']['end']:\n",
    "            break\n",
    "        epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(f'../ckpt_{config[\"name\"]}/epoch_{config[\"test\"][\"epoch\"]}.pt', map_location=torch.device(device))\n",
    "criterion = EnergyLossVectorized()\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = pd.read_csv('ground_truth_loss.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "folder_name = f'{config[\"test\"][\"name\"]}_test'\n",
    "test_loss = []\n",
    "test_loss_ratio=[]\n",
    "for test_idx in tqdm(range(10000, 11000)):\n",
    "    G_vis = G_list[test_idx]\n",
    "    node_pos,loss = evaluate(model, data_list[test_idx],criterion,device)\n",
    "    gt_loss = ground_truth.loc[test_idx][0]\n",
    "    loss_ratio = (loss - gt_loss) / gt_loss\n",
    "    test_loss.append(loss)\n",
    "    test_loss_ratio.append(loss_ratio)\n",
    "    graph_vis(G_vis, node_pos, f'{folder_name}/{config[\"test\"][\"out_prefix\"]}_model_{test_idx}_{loss}_{loss_ratio}.png') \n",
    "#     node_pos = nx.nx_agraph.graphviz_layout(G_vis, prog='neato')\n",
    "#     plt.figure()\n",
    "#     nx.draw(G_vis, node_pos)\n",
    "#     plt.savefig(f'{folder_name}/{test_idx}.png')\n",
    "\n",
    "print(np.nanmean(test_loss), np.nanstd(test_loss))\n",
    "\n",
    "print(np.nanmean(test_loss_ratio), np.nanstd(test_loss_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "loss_ratios = []\n",
    "for test_idx in tqdm(range(10000, 11000)):\n",
    "    G_vis = G_list[test_idx]\n",
    "    node_pos, loss = evaluate(model, data_list[test_idx], criterion, device)\n",
    "    gt_loss = ground_truth.loc[test_idx][0]\n",
    "    loss_ratio = (loss - gt_loss) / gt_loss\n",
    "    losses += [loss]\n",
    "    loss_ratios += [loss_ratio]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(losses), np.std(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(loss_ratios), np.std(loss_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_loss = 0\n",
    "pred_loss = 0\n",
    "for idx in tqdm(range(10000, 11000)):\n",
    "    pred, loss = evaluate(model, data_list[idx], criterion, device)\n",
    "    pos_map = nx.nx_agraph.graphviz_layout(G_list[idx], prog='neato')\n",
    "\n",
    "    pred_mean, pred_std = pred.mean(axis=0), pred.std()\n",
    "    truth = np.array(list(pos_map.values()))\n",
    "    truth_mean, truth_std = truth.mean(axis=0), truth.std()\n",
    "    norm_truth = (truth - truth_mean) / truth_std\n",
    "    scaled_truth = norm_truth * pred_std + pred_mean\n",
    "\n",
    "    truth_loss += criterion(torch.tensor(scaled_truth), data_list[idx])\n",
    "    pred_loss += criterion(torch.tensor(pred), data_list[idx])\n",
    "    \n",
    "truth_loss / 1000, pred_loss / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data_list[9999].x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 5\n",
    "losses = []\n",
    "folder_name = f'{config[\"test\"][\"name\"]}_iterative_test'\n",
    "for test_idx in tqdm(range(10000, len(G_list))):\n",
    "    G_vis = G_list[test_idx]\n",
    "    node_pos = nx.nx_agraph.graphviz_layout(G_vis, prog='neato')\n",
    "    plt.figure()\n",
    "    nx.draw(G_vis, node_pos)\n",
    "    plt.savefig(f'{folder_name}/{test_idx}.png')\n",
    "    for i in range(iterations):\n",
    "        node_pos, loss = evaluate(model, data_list[test_idx], criterion, device) \n",
    "        data_list[test_idx].x = torch.tensor(node_pos,dtype=torch.float)\n",
    "    losses += [loss]\n",
    "    graph_vis(G_vis, node_pos, f'{folder_name}/{config[\"test\"][\"out_prefix\"]}_iter_model_{test_idx}_{loss}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(loss_ep[:1000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnergyLossScaled(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, p, data, scale):\n",
    "        edge_attr = data.edge_attr\n",
    "        # convert per-node positions to per-edge positions\n",
    "        start, end, n_nodes = node2edge(p, data)\n",
    "        \n",
    "        start *= scale\n",
    "        end *= scale\n",
    "        \n",
    "        start_x = start[:, 0]\n",
    "        start_y = start[:, 1]\n",
    "        end_x = end[:, 0]\n",
    "        end_y = end[:, 1]\n",
    "        \n",
    "        l = edge_attr[:, 0]\n",
    "        k = edge_attr[:, 1]\n",
    "        \n",
    "        term1 = (start_x - end_x) ** 2\n",
    "        term2 = (start_y - end_y) ** 2\n",
    "        term3 = l ** 2\n",
    "        term4 = 2 * l * (term1 + term2).sqrt()\n",
    "        energy = k / 2 * (term1 + term2 + term3 - term4)\n",
    "        return energy.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_scaled = EnergyLossScaled()\n",
    "criterion = EnergyLossVectorized()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_loss = 0\n",
    "pred_loss = 0\n",
    "for idx in tqdm(range(10000, 11000)):\n",
    "    pred, loss = evaluate(model, data_list[idx], criterion, device)\n",
    "    pos_map = nx.nx_agraph.graphviz_layout(G_list[idx], prog='neato')\n",
    "\n",
    "    pred_mean, pred_std = pred.mean(axis=0), pred.std()\n",
    "    truth = np.array(list(pos_map.values()))\n",
    "    truth_mean, truth_std = truth.mean(axis=0), truth.std()\n",
    "    norm_truth = (truth - truth_mean) / truth_std\n",
    "    scaled_truth = norm_truth * pred_std + pred_mean\n",
    "\n",
    "    truth_loss += criterion(torch.tensor(scaled_truth, device=device), data_list[idx])\n",
    "    pred_loss += criterion(torch.tensor(pred, device=device), data_list[idx])\n",
    "    \n",
    "truth_loss / 1000, pred_loss / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in [10898, 10904]:#tqdm(range(10000, 11000)):\n",
    "    data, G = data_list[idx], G_list[idx]\n",
    "    edge_attr = data.edge_attr\n",
    "    pred, loss = evaluate(model, data, criterion, device)\n",
    "    pos_map = nx.nx_agraph.graphviz_layout(G, prog='neato')\n",
    "    truth = np.array(list(pos_map.values()))\n",
    "\n",
    "    start, end, n_nodes = node2edge(torch.tensor(truth, device=device), data)\n",
    "    w = edge_attr[:, 1]\n",
    "    d = edge_attr[:, 0]\n",
    "\n",
    "    u2 = ((start - end) ** 2).sum(dim=1)\n",
    "\n",
    "    s = (w * d * u2.sqrt()).sum() / (w * u2).sum()\n",
    "\n",
    "    loss_gt = criterion_scaled(torch.tensor(truth, device=device), data, s)\n",
    "\n",
    "    print(loss, loss_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data, criterion, device, idx):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        data = data.to(device)\n",
    "        pred = model(data).detach()\n",
    "        loss = criterion(pred,data).cpu().numpy()\n",
    "        loss = round(float(loss),2)\n",
    "    return pred.cpu().numpy(), loss\n",
    "\n",
    "def graph_vis(G, node_pos):\n",
    "    i = 0\n",
    "    for n, p in node_pos:\n",
    "        node = 'n' +str(i)\n",
    "        G.nodes[node]['pos'] = (n,p)\n",
    "        i += 1\n",
    "    pos = nx.get_node_attributes(G,'pos')\n",
    "    plt.figure()\n",
    "    nx.draw(G, pos)\n",
    "    \n",
    "for test_idx in tqdm(list(range(10000, len(data_list)))):\n",
    "    G_vis = G_list[test_idx]\n",
    "    node_pos,loss = evaluate(model, data_list[test_idx],criterion,device, test_idx)\n",
    "    if loss > 10000:\n",
    "        print(test_idx, loss, data_list[test_idx].num_nodes)\n",
    "        graph_vis(G_vis, node_pos) \n",
    "        node_pos = nx.nx_agraph.graphviz_layout(G_vis, prog='neato')\n",
    "        plt.figure()\n",
    "        nx.draw(G_vis, node_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
