{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wZtYFEhHoBGp"
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KJPfzpxLQI5O"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from deepgd import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z4eBEyB1oaJe"
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Q_-MLFhpNvxF"
   },
   "outputs": [],
   "source": [
    "cuda_idx = 0\n",
    "canonicalization = IdentityTransformation()\n",
    "config = StaticConfig({\n",
    "    \"name\": 'deepgd(conv=gat_simple(self_loop=false,heads=4))',\n",
    "    \"uid\": None,\n",
    "    \"link\": None,\n",
    "    \"generator\": {\n",
    "        \"params\": {\n",
    "            \"num_blocks\": 9,\n",
    "            \"normalize\": canonicalization\n",
    "        },\n",
    "        \"pretrained\": {\n",
    "            \"name\": None,\n",
    "            \"epoch\": -1,\n",
    "        },\n",
    "        \"optim\": torch.optim.AdamW,\n",
    "        \"lr\" : {\n",
    "            \"initial\": 1e-3,\n",
    "            \"decay\": 0.99,\n",
    "        },\n",
    "    },\n",
    "    \"discriminator\": {\n",
    "        \"params\": {\n",
    "            \"conv\": [2, 16, 16, 16],\n",
    "            \"dense\": [2],\n",
    "            \"shared_depth\": 10,\n",
    "            \"enet_depth\": 6,\n",
    "            \"enet_width\": 64,\n",
    "            \"aggr\": \"add\",\n",
    "            \"normalize\": canonicalization\n",
    "        },\n",
    "        \"pretrained\": {\n",
    "            \"name\": None,\n",
    "            \"epoch\": -1,\n",
    "        },\n",
    "        \"optim\": torch.optim.AdamW,\n",
    "        \"lr\" : {\n",
    "            \"initial\": 1e-3,\n",
    "            \"decay\": 0.99,\n",
    "        },\n",
    "        \"noise\": {\n",
    "            \"std\": 0,\n",
    "            \"decay\": 0.95,\n",
    "        },\n",
    "        \"repeat\": 1,\n",
    "        \"complete\": True,\n",
    "        \"adaptive\": True\n",
    "    },\n",
    "    \"alternate\": \"epoch\",\n",
    "    \"batchsize\": 128,\n",
    "    \"epoch\": {\n",
    "        \"start\": -1,\n",
    "        \"end\": None,\n",
    "    },\n",
    "    \"log_interval\": 1,\n",
    "    \"test\": {\n",
    "        \"name\": \"test\",\n",
    "        \"epoch\": -1,\n",
    "    },\n",
    "    \"gan_flavor\": \"dgdv2\",\n",
    "    \"gp_weight\": 0,\n",
    "})\n",
    "data_config = StaticConfig({\n",
    "    \"sparse\": False,\n",
    "    \"pivot\": None,\n",
    "    \"init\": \"pmds\",\n",
    "    \"edge\": {\n",
    "        \"index\": \"full_edge_index\",\n",
    "        \"attr\": \"full_edge_attr\",\n",
    "    },\n",
    "})\n",
    "loss_fns = {\n",
    "    Stress(): 1\n",
    "}\n",
    "ctrler_params = {\n",
    "    \"tau\": 0.95,\n",
    "    \"beta\": 1,\n",
    "    \"exploit_rate\": 0.5,\n",
    "    \"warmup\": 2,\n",
    "}\n",
    "paths = StaticConfig({\n",
    "    \"root\": \"artifacts\",\n",
    "    \"checkpoints\": lambda: f\"{paths.root}/checkpoints/{config.name}\",\n",
    "    \"gen_pretrain\": lambda: f\"{paths.root}/checkpoints/{config.generator.pretrained.name}\",\n",
    "    \"dis_pretrain\": lambda: f\"{paths.root}/checkpoints/{config.discriminator.pretrained.name}\",\n",
    "    \"tensorboard\": lambda: f\"{paths.root}/tensorboards/{config.name}\",\n",
    "    \"visualization\": lambda: f\"{paths.root}/visualizations/{config.name}_{config.test.name}\",\n",
    "    \"log\": lambda: f\"{paths.root}/logs/{config.name}.log\",\n",
    "    \"metrics\": lambda suffix: f\"{paths.root}/metrics/{config.name}_{suffix}.pickle\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mdMEAbH10Qaq"
   },
   "outputs": [],
   "source": [
    "if \" \" in config.name:\n",
    "    raise Exception(\"Space is not allowed in model name.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WynW4ZAdBhep"
   },
   "source": [
    "# Prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DR6-vYtr_i_P"
   },
   "source": [
    "## Get log command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9-QpinlslcTO",
    "outputId": "b71f2ad3-e3f3-4ee1-e8ac-58e6e414abfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cd /users/PAS0027/osu10203/deepgd && tail -n1000 -f 'artifacts/logs/deepgd(conv=gat_simple(self_loop=false,heads=4)).log'\n"
     ]
    }
   ],
   "source": [
    "print(f\"cd {os.getcwd()} && tail -n1000 -f '{paths.log()}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard dev upload --logdir 'artifacts/tensorboards/deepgd(conv=gat_simple(self_loop=false,heads=4))'\n"
     ]
    }
   ],
   "source": [
    "print(f\"tensorboard dev upload --logdir '{paths.tensorboard()}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "innqkwvH_ydD"
   },
   "source": [
    "## Set globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Qjq7JlSZlkQR"
   },
   "outputs": [],
   "source": [
    "if cuda_idx is not None and torch.cuda.is_available():\n",
    "    device = f'cuda:{cuda_idx}'\n",
    "    pynvml.nvmlInit()\n",
    "    cuda = pynvml.nvmlDeviceGetHandleByIndex(cuda_idx)\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    cuda =  None\n",
    "np.set_printoptions(precision=2)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0da286l_ApEL"
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RFuSHiZJMU4x",
    "outputId": "3befa34f-6262-47a9-825d-7dcb2d6596d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load from 'cache/G_list.pickle'\n",
      "Load from 'cache/generate_data_list(list,sparse=False,pivot_mode=None,init_mode=pmds,edge_index=full_edge_index,edge_attr=full_edge_attr,pmds_list=ndarray,gviz_list=ndarray,noisy_layout=True,device=cpu).pickle'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/PAS0027/osu10203/.conda/envs/deepgd/lib/python3.9/site-packages/torch_geometric/deprecation.py:13: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "G_list = load_G_list(data_path='data/rome', index_file='data_index.txt', cache='G_list', cache_prefix='cache/')\n",
    "data_list = generate_data_list(G_list, \n",
    "                               sparse=data_config.sparse, \n",
    "                               pivot_mode=data_config.pivot,\n",
    "                               init_mode=data_config.init,\n",
    "                               edge_index=data_config.edge.index,\n",
    "                               edge_attr=data_config.edge.attr,\n",
    "                               pmds_list=np.load('layouts/rome/pmds.npy', allow_pickle=True),\n",
    "                               gviz_list=np.load('layouts/rome/gviz.npy', allow_pickle=True),\n",
    "                               noisy_layout=True,\n",
    "                               device='cpu', \n",
    "                               cache=True,\n",
    "                               cache_prefix='cache/')\n",
    "train_loader = LazyDeviceMappingDataLoader(data_list[:10000], batch_size=config.batchsize, shuffle=True, device=device)\n",
    "val_loader = LazyDeviceMappingDataLoader(data_list[11000:], batch_size=config.batchsize, shuffle=False, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-ODE2k8BFV6"
   },
   "source": [
    "## Create folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "3ykUMINYBRog"
   },
   "outputs": [],
   "source": [
    "mkdirs(paths.checkpoints(), paths.tensorboard(), paths.visualization(), f\"{paths.root}/logs\", f\"{paths.root}/metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFq9k5nzBIh5"
   },
   "source": [
    "## Load checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATConvWithEFeat(gnn.GATConv):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels, \n",
    "                 efeat_dim,\n",
    "                 heads: int = 4,\n",
    "                 concat: bool = False,\n",
    "                 negative_slope: float = 0.2,\n",
    "                 dropout: float = 0.0,\n",
    "                 add_self_loops: bool = False,\n",
    "                 bias: bool = True,\n",
    "                 **kwargs):\n",
    "        super().__init__(in_channels=in_channels,\n",
    "                         out_channels=out_channels, \n",
    "                         heads=heads,\n",
    "                         concat=concat,\n",
    "                         negative_slope=negative_slope,\n",
    "                         dropout=dropout,\n",
    "                         add_self_loops=add_self_loops,\n",
    "                         bias=bias, **kwargs)\n",
    "        self.att_efeat = nn.Parameter(torch.Tensor(1, heads, efeat_dim))\n",
    "        gnn.inits.glorot(self.att_efeat)\n",
    "\n",
    "    def forward(self, x, edge_index, e, size=None, return_attention_weights=None):\n",
    "        H, C = self.heads, self.out_channels\n",
    "\n",
    "        # We first transform the input node features. If a tuple is passed, we\n",
    "        # transform source and target node features via separate weights:\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            assert x.dim() == 2, \"Static graphs not supported in 'GATConv'\"\n",
    "            x_src = x_dst = self.lin_src(x).view(-1, H, C)\n",
    "        else:  # Tuple of source and target node features:\n",
    "            x_src, x_dst = x\n",
    "            assert x_src.dim() == 2, \"Static graphs not supported in 'GATConv'\"\n",
    "            x_src = self.lin_src(x_src).view(-1, H, C)\n",
    "            if x_dst is not None:\n",
    "                x_dst = self.lin_dst(x_dst).view(-1, H, C)\n",
    "\n",
    "        x = (x_src, x_dst)\n",
    "\n",
    "        # Next, we compute node-level attention coefficients, both for source\n",
    "        # and target nodes (if present):\n",
    "        alpha_src = (x_src * self.att_src).sum(dim=-1)\n",
    "        alpha_dst = None if x_dst is None else (x_dst * self.att_dst).sum(-1)\n",
    "        alpha = (alpha_src, alpha_dst)\n",
    "\n",
    "        if self.add_self_loops:\n",
    "            if isinstance(edge_index, torch.Tensor):\n",
    "                # We only want to add self-loops for nodes that appear both as\n",
    "                # source and target nodes:\n",
    "                num_nodes = x_src.size(0)\n",
    "                if x_dst is not None:\n",
    "                    num_nodes = min(num_nodes, x_dst.size(0))\n",
    "                num_nodes = min(size) if size is not None else num_nodes\n",
    "                edge_index, _ = pyg.utils.remove_self_loops(edge_index)\n",
    "                edge_index, _ = pyg.utils.add_self_loops(edge_index, num_nodes=num_nodes)\n",
    "            elif isinstance(edge_index, torch_sparse.SparseTensor):\n",
    "                edge_index = set_diag(edge_index)\n",
    "\n",
    "        # propagate_type: (x: OptPairTensor, alpha: OptPairTensor)\n",
    "        out = self.propagate(edge_index, x=x, e=e, alpha=alpha, size=size)\n",
    "\n",
    "        alpha = self._alpha\n",
    "        assert alpha is not None\n",
    "        self._alpha = None\n",
    "\n",
    "        if self.concat:\n",
    "            out = out.view(-1, self.heads * self.out_channels)\n",
    "        else:\n",
    "            out = out.mean(dim=1)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out += self.bias\n",
    "\n",
    "        if isinstance(return_attention_weights, bool):\n",
    "            if isinstance(edge_index, torch.Tensor):\n",
    "                return out, (edge_index, alpha)\n",
    "            elif isinstance(edge_index, torch_sparse.SparseTensor):\n",
    "                return out, edge_index.set_value(alpha, layout='coo')\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "\n",
    "    def message(self, x_j, e, alpha_j, alpha_i, index, ptr, size_i):\n",
    "        # Given egel-level attention coefficients for source and target nodes,\n",
    "        # we simply need to sum them up to \"emulate\" concatenation:\n",
    "        alpha = alpha_j if alpha_i is None else alpha_j + alpha_i\n",
    "#         print(e.shape, self.att_efeat.shape)\n",
    "        alpha += (e[:, None, :].repeat(1, self.heads, 1) * self.att_efeat).sum(dim=-1)\n",
    "\n",
    "        alpha = F.leaky_relu(alpha, self.negative_slope)\n",
    "        alpha = pyg.utils.softmax(alpha, index, ptr, size_i)\n",
    "        self._alpha = alpha  # Save for later use.\n",
    "        alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n",
    "        return x_j * alpha.unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GNNLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 nfeat_dims,\n",
    "                 efeat_dim,\n",
    "                 aggr,\n",
    "                 edge_net=None, \n",
    "                 dense=False,\n",
    "                 bn=True, \n",
    "                 act=True, \n",
    "                 dp=None,\n",
    "                 root_weight=True,\n",
    "                 skip=True):\n",
    "        super().__init__()\n",
    "        try:\n",
    "            in_dim = nfeat_dims[0]\n",
    "            out_dim = nfeat_dims[1]\n",
    "        except:\n",
    "            in_dim = nfeat_dims\n",
    "            out_dim = nfeat_dims\n",
    "        self.conv = GATConvWithEFeat(in_dim, out_dim, efeat_dim, aggr=aggr)\n",
    "        self.dense = nn.Linear(out_dim, out_dim) if dense else nn.Identity()\n",
    "        self.bn = gnn.BatchNorm(out_dim) if bn else nn.Identity()\n",
    "        self.act = nn.LeakyReLU() if act else nn.Identity()\n",
    "        self.dp = dp and nn.Dropout(dp) or nn.Identity()\n",
    "        self.skip = skip\n",
    "        self.proj = nn.Linear(in_dim, out_dim, bias=False) if in_dim != out_dim else nn.Identity()\n",
    "        \n",
    "    def forward(self, v, e, data):\n",
    "        v_ = v\n",
    "        v = self.conv(v, data.edge_index, e)\n",
    "        v = self.dense(v)\n",
    "        v = self.bn(v)\n",
    "        v = self.act(v)\n",
    "        v = self.dp(v)\n",
    "        return v + self.proj(v_) if self.skip else v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GNNBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "                 feat_dims, \n",
    "                 efeat_hid_dims=[], \n",
    "                 efeat_hid_act=nn.LeakyReLU,\n",
    "                 efeat_out_act=nn.Tanh,\n",
    "                 bn=False,\n",
    "                 act=True,\n",
    "                 dp=None,\n",
    "                 aggr='mean',\n",
    "                 root_weight=True,\n",
    "                 static_efeats=1,\n",
    "                 dynamic_efeats='skip',\n",
    "                 rich_efeats=False,\n",
    "                 euclidian=False,\n",
    "                 direction=False,\n",
    "                 n_weights=0,\n",
    "                 residual=False):\n",
    "        '''\n",
    "        dynamic_efeats: {\n",
    "            skip: block input to each layer, \n",
    "            first: block input to first layer, \n",
    "            prev: previous layer output to next layer, \n",
    "            orig: original node feature to each layer\n",
    "        }\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.static_efeats = static_efeats\n",
    "        self.dynamic_efeats = dynamic_efeats\n",
    "        self.rich_efeats = rich_efeats\n",
    "        self.euclidian = euclidian\n",
    "        self.direction = direction\n",
    "        self.n_weights = n_weights\n",
    "        self.residual = residual\n",
    "        self.gnn = nn.ModuleList()\n",
    "        self.n_layers = len(feat_dims) - 1\n",
    "\n",
    "        for idx, (in_feat, out_feat) in enumerate(zip(feat_dims[:-1], feat_dims[1:])):\n",
    "            direction_dim = (feat_dims[idx] if self.dynamic_efeats == 'prev'\n",
    "                             else 2 if self.dynamic_efeats == 'orig'\n",
    "                             else feat_dims[0])\n",
    "            in_efeat_dim = self.static_efeats\n",
    "            if self.dynamic_efeats != 'first': \n",
    "                in_efeat_dim += self.euclidian + self.direction * direction_dim + self.n_weights + 3 * self.rich_efeats\n",
    "            edge_net = nn.Sequential(*chain.from_iterable(\n",
    "                [nn.Linear(idim, odim),\n",
    "                 nn.BatchNorm1d(odim),\n",
    "                 act()]\n",
    "                for idim, odim, act in zip([in_efeat_dim] + efeat_hid_dims,\n",
    "                                           efeat_hid_dims + [in_feat * out_feat],\n",
    "                                           [efeat_hid_act] * len(efeat_hid_dims) + [efeat_out_act])\n",
    "            ))\n",
    "            self.gnn.append(GNNLayer(nfeat_dims=(in_feat, out_feat), \n",
    "                                     efeat_dim=in_efeat_dim, \n",
    "                                     edge_net=edge_net,\n",
    "                                     bn=bn, \n",
    "                                     act=act, \n",
    "                                     dp=dp,\n",
    "                                     aggr=aggr,\n",
    "                                     root_weight=root_weight,\n",
    "                                     skip=False))\n",
    "        \n",
    "    def _get_edge_feat(self, pos, data, rich_efeats=False, euclidian=False, direction=False, weights=None):\n",
    "        e = data.edge_attr[:, :self.static_efeats]\n",
    "        if euclidian or direction:\n",
    "            start_pos, end_pos = get_edges(pos, data)\n",
    "            v, u = l2_normalize(end_pos - start_pos, return_norm=True)\n",
    "            if euclidian:\n",
    "                e = torch.cat([e, u], dim=1)\n",
    "            if direction:\n",
    "                e = torch.cat([e, v], dim=1)\n",
    "            if rich_efeats:\n",
    "                d = e[:, :1]\n",
    "                d2 = d ** 2\n",
    "#                 d_inv = 1 / d\n",
    "#                 d2_inv = 1 / d2\n",
    "                u2 = u ** 2\n",
    "                ud = u * d\n",
    "#                 u_inv = 1 / u\n",
    "#                 u2_inv = 1 / u2\n",
    "                e = torch.cat([e, d2, u2, ud], dim=1)\n",
    "        if weights is not None:\n",
    "            w = weights.repeat(len(e), 1)\n",
    "            e = torch.cat([e, w], dim=1)\n",
    "        return e\n",
    "    \n",
    "    def _get_dynamic_edge_feat(self, pos, data, rich_efeats=False, euclidian=False, direction=False, weights=None):\n",
    "        if euclidian or direction:\n",
    "            start_pos, end_pos = get_edges(pos, data)\n",
    "            d, u = l2_normalize(end_pos - start_pos, return_norm=True)\n",
    "            if euclidian and direction:\n",
    "                e = torch.cat([u, d], dim=1)\n",
    "            else:\n",
    "                if euclidian:\n",
    "                    e = u\n",
    "                if direction:\n",
    "                    e = d\n",
    "        if weights is not None:\n",
    "            w = weights.repeat(len(e), 1)\n",
    "            e = torch.cat([e, w], dim=1)\n",
    "        return e\n",
    "        \n",
    "    def forward(self, v, data, weights=None):\n",
    "        vres = v\n",
    "        for layer in range(self.n_layers):\n",
    "            vsrc = (v if self.dynamic_efeats == 'prev' \n",
    "                    else data.pos if self.dynamic_efeats == 'orig' \n",
    "                    else vres)\n",
    "            get_extra = not (self.dynamic_efeats == 'first' and layer != 0)\n",
    "            efeat_fn = self._get_dynamic_edge_feat if self.static_efeats == 0 else self._get_edge_feat\n",
    "\n",
    "            e = efeat_fn(vsrc, data,\n",
    "                         rich_efeats=self.rich_efeats and get_extra,\n",
    "                         euclidian=self.euclidian and get_extra, \n",
    "                         direction=self.direction and get_extra,\n",
    "                         weights=weights if get_extra and self.n_weights > 0 else None)\n",
    "            v = self.gnn[layer](v, e, data)\n",
    "        return v + vres if self.residual else v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_blocks=9, \n",
    "                 num_layers=3,\n",
    "                 num_enet_layers=2,\n",
    "                 layer_dims=None,\n",
    "                 n_weights=0, \n",
    "                 dynamic_efeats='skip',\n",
    "                 euclidian=True,\n",
    "                 direction=True,\n",
    "                 residual=True,\n",
    "                 normalize=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_blocks = nn.ModuleList([\n",
    "            GNNBlock(feat_dims=[2, 8, 8 if layer_dims is None else layer_dims[0]], bn=True, dp=0.2, static_efeats=2)\n",
    "        ])\n",
    "        self.hid_blocks = nn.ModuleList([\n",
    "            GNNBlock(feat_dims=layer_dims or ([8] + [8] * num_layers), \n",
    "                     efeat_hid_dims=[16] * (num_enet_layers - 1),\n",
    "                     bn=True, \n",
    "                     act=True,\n",
    "                     dp=0.2, \n",
    "                     static_efeats=2,\n",
    "                     dynamic_efeats=dynamic_efeats,\n",
    "                     euclidian=euclidian,\n",
    "                     direction=direction,\n",
    "                     n_weights=n_weights,\n",
    "                     residual=residual)\n",
    "            for _ in range(num_blocks)\n",
    "        ])\n",
    "        self.out_blocks = nn.ModuleList([\n",
    "            GNNBlock(feat_dims=[8 if layer_dims is None else layer_dims[-1], 8], bn=True, static_efeats=2),\n",
    "            GNNBlock(feat_dims=[8, 2], act=False, static_efeats=2)\n",
    "        ])\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def forward(self, data, weights=None, output_hidden=False, numpy=False):\n",
    "        v = data.pos if data.pos is not None else generate_rand_pos(len(data.x)).to(data.x.device)\n",
    "        if self.normalize is not None:\n",
    "            v = self.normalize(v, data)\n",
    "        \n",
    "        hidden = []\n",
    "        for block in chain(self.in_blocks, \n",
    "                           self.hid_blocks, \n",
    "                           self.out_blocks):\n",
    "            v = block(v, data, weights)\n",
    "            if output_hidden:\n",
    "                hidden.append(v.detach().cpu().numpy() if numpy else v)\n",
    "        if not output_hidden:\n",
    "            vout = v.detach().cpu().numpy() if numpy else v\n",
    "            if self.normalize is not None:\n",
    "                vout = self.normalize(vout, data)\n",
    "        \n",
    "        return hidden if output_hidden else vout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "96ZActxlL0fJ"
   },
   "outputs": [],
   "source": [
    "class StressDiscriminator(nn.Module):\n",
    "    def __init__(self, normalize=CanonicalizationByStress(), **kwargs):\n",
    "        super().__init__()\n",
    "        self.dummy = nn.Parameter(torch.zeros(1))\n",
    "        self.normalize = normalize\n",
    "        self.stress = Stress(reduce=None)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        return -self.stress(self.normalize(batch.pos, batch), batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "IJ6xCYBUuhcw"
   },
   "outputs": [],
   "source": [
    "def get_ckpt_epoch(folder, epoch):\n",
    "    if not os.path.isdir(folder):\n",
    "        os.mkdir(folder)\n",
    "    if epoch >= 0:\n",
    "        return epoch\n",
    "    ckpt_files = os.listdir(folder)\n",
    "    last_epoch = 0\n",
    "    if ckpt_files:\n",
    "        last_epoch = sorted(list(map(lambda x: int(re.search('(?<=epoch_)(\\d+)(?=\\.)', x).group(1)), ckpt_files)))[-1]\n",
    "    return last_epoch + epoch + 1\n",
    "\n",
    "def start_epoch():\n",
    "    return get_ckpt_epoch(paths.checkpoints(), config.epoch.start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JacDYXka9U01",
    "outputId": "8a0d9c75-eb27-4cfd-eb3e-c81b698cb086"
   },
   "outputs": [],
   "source": [
    "generator = Generator(**config.generator.params[...]).to(device)\n",
    "generator_optimizer = config.generator.optim(generator.parameters(), lr=config.generator.lr.initial * config.generator.lr.decay ** start_epoch())\n",
    "generator_scheduler = torch.optim.lr_scheduler.ExponentialLR(generator_optimizer, gamma=config.generator.lr.decay)\n",
    "if start_epoch() != 0:\n",
    "    gen_ckpt_epoch = start_epoch()\n",
    "elif config.generator.pretrained.name is not None and config.generator.pretrained.epoch != 0:\n",
    "    gen_pretrained_epoch = get_ckpt_epoch(paths.gen_pretrain(), config.generator.pretrained.epoch)\n",
    "    gen_ckpt_epoch = gen_pretrained_epoch \n",
    "else:\n",
    "    gen_ckpt_epoch = None\n",
    "if gen_ckpt_epoch is not None:\n",
    "    # Load generator\n",
    "    gen_ckpt_file = f\"{paths.checkpoints()}/gen_epoch_{gen_ckpt_epoch}.pt\"\n",
    "    print(f\"Loading from {gen_ckpt_file}...\")\n",
    "    generator.load_state_dict(torch.load(gen_ckpt_file, map_location=torch.device(device)))\n",
    "    # Load generator optimizer\n",
    "    gen_optim_ckpt_file = f\"{paths.checkpoints()}/gen_optim_epoch_{gen_ckpt_epoch}.pt\"\n",
    "    print(f\"Loading from {gen_optim_ckpt_file}...\")\n",
    "    generator_optimizer.load_state_dict(torch.load(gen_optim_ckpt_file, map_location=torch.device(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oWQhD7-88VXe",
    "outputId": "3ce11a68-adca-4f26-b71c-69fb71d301d2"
   },
   "outputs": [],
   "source": [
    "discriminator = StressDiscriminator(**config.discriminator.params[...]).to(device)\n",
    "# discriminator_optimizer = config.discriminator.optim(discriminator.parameters(), lr=config.discriminator.lr.initial * config.discriminator.lr.decay ** start_epoch())\n",
    "# discriminator_scheduler = torch.optim.lr_scheduler.ExponentialLR(discriminator_optimizer, gamma=config.discriminator.lr.decay)\n",
    "# if start_epoch() != 0:\n",
    "#     dis_ckpt_epoch = start_epoch()\n",
    "# elif config.discriminator.pretrained.name is not None and config.discriminator.pretrained.epoch != 0:\n",
    "#     dis_pretrained_epoch = get_ckpt_epoch(paths.dis_pretrain(), config.discriminator.pretrained.epoch)\n",
    "#     dis_ckpt_epoch = dis_pretrained_epoch # f\"{paths.dis_pretrain()}/dis_epoch_{dis_pretrained_epoch}.pt\"\n",
    "# else:\n",
    "#     dis_ckpt_epoch = None\n",
    "# if dis_ckpt_epoch is not None:\n",
    "#     # Load discriminator\n",
    "#     dis_ckpt_file = f\"{paths.checkpoints()}/dis_epoch_{dis_ckpt_epoch}.pt\"\n",
    "#     print(f\"Loading from {dis_ckpt_file}...\")\n",
    "#     discriminator.load_state_dict(torch.load(dis_ckpt_file, map_location=torch.device(device)))\n",
    "#     # Load discriminator optimizer\n",
    "#     dis_optim_ckpt_file = f\"{paths.checkpoints()}/dis_optim_epoch_{dis_ckpt_epoch}.pt\"\n",
    "#     print(f\"Loading from {dis_optim_ckpt_file}...\")\n",
    "#     discriminator_optimizer.load_state_dict(torch.load(dis_optim_ckpt_file, map_location=torch.device(device)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N39dDHraedM6"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "BdIJSDSJedM-"
   },
   "outputs": [],
   "source": [
    "stress_criterion = StressDiscriminator().to(device)\n",
    "val_criterion = Stress(reduce=None)\n",
    "xing_criterion = Xing(reduce=None)\n",
    "dis_convert = DiscriminatorDataConverter(complete_graph=config.discriminator.complete, normalize=config.discriminator.params.normalize)\n",
    "tensorboard = SummaryWriter(log_dir=paths.tensorboard())\n",
    "epoch = start_epoch() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "OHRWj1h0edM-"
   },
   "outputs": [],
   "source": [
    "def gradient_penalty(interpolated, discriminator, weight=10):\n",
    "    interpolated.pos.requires_grad_()\n",
    "    prob_interpolated = discriminator(interpolated)\n",
    "    gradients = autograd.grad(outputs=prob_interpolated, \n",
    "                              inputs=interpolated.pos,\n",
    "                              grad_outputs=torch.ones_like(prob_interpolated),\n",
    "                              create_graph=True, \n",
    "                              retain_graph=True, \n",
    "                              allow_unused=True)[0]\n",
    "    gradients_norm = torch.sqrt(gnn.global_add_pool(gradients.square().sum(dim=1), batch.batch) + 1e-8)\n",
    "    return weight * ((gradients_norm - 1) ** 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "o5wJJN9AedM_"
   },
   "outputs": [],
   "source": [
    "def get_gp_loss(batch, fake_pos, weight):\n",
    "    if weight > 0:\n",
    "        interp = dis_convert(batch, fake_pos, random.random())\n",
    "        return gradient_penalty(interp, discriminator, weight).mean()\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "QwuA6sRoedM_"
   },
   "outputs": [],
   "source": [
    "def get_sgan_loss(batch, fake_pos, mode='discriminator'):\n",
    "    real = dis_convert(batch)\n",
    "    fake = dis_convert(batch, fake_pos)\n",
    "    pred = discriminator(merge_batch(real, fake)).view(2, -1).T\n",
    "    if mode == 'discriminator':\n",
    "        label = torch.zeros(pred.shape[0]).long()\n",
    "    elif mode == 'generator':\n",
    "        label = torch.ones(pred.shape[0]).long()\n",
    "    else:\n",
    "        raise Exception\n",
    "    return nn.CrossEntropyLoss()(pred, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "GuAP1YchedNA"
   },
   "outputs": [],
   "source": [
    "def get_rgan_loss(batch, fake_pos, mode='discriminator'):\n",
    "    real = dis_convert(batch)\n",
    "    fake = dis_convert(batch, fake_pos)\n",
    "    pred = discriminator(merge_batch(real, fake)).view(2, -1).T\n",
    "    real_pred, fake_pred = pred[:,0], pred[:,1]\n",
    "    if mode == 'discriminator':\n",
    "        losses = - F.logsigmoid(real_pred - fake_pred)\n",
    "    elif mode == 'generator':\n",
    "        losses = - F.logsigmoid(fake_pred - real_pred)\n",
    "    else:\n",
    "        raise Exception\n",
    "    return losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "KFllQ7BredNA"
   },
   "outputs": [],
   "source": [
    "def get_wgan_loss(batch, fake_pos, mode='discriminator'):\n",
    "    real = dis_convert(batch)\n",
    "    fake = dis_convert(batch, fake_pos)\n",
    "    pred = discriminator(merge_batch(real, fake)).view(2, -1).T\n",
    "    real_pred, fake_pred = pred[:,0], pred[:,1]\n",
    "    if mode == 'discriminator':\n",
    "        losses = fake_pred - real_pred \n",
    "    elif mode == 'generator':\n",
    "        losses = real_pred - fake_pred\n",
    "    else:\n",
    "        raise Exception\n",
    "    return losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "r5Kg_tYEedNA"
   },
   "outputs": [],
   "source": [
    "def get_ragan_loss(batch, fake_pos, mode='discriminator'):\n",
    "    real = dis_convert(batch)\n",
    "    fake = dis_convert(batch, fake_pos)\n",
    "    pred = discriminator(merge_batch(real, fake)).view(2, -1).T\n",
    "    real_pred, fake_pred = pred[:,0], pred[:,1]\n",
    "    if mode == 'discriminator':\n",
    "        losses = - F.logsigmoid(real_pred - fake_pred.mean()) - F.logsigmoid(real_pred.mean() - fake_pred)\n",
    "    elif mode == 'generator':\n",
    "        losses = - F.logsigmoid(fake_pred - real_pred.mean()) - F.logsigmoid(fake_pred.mean() - real_pred)\n",
    "    else:\n",
    "        raise Exception\n",
    "    return losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dgdv2_loss(batch, fake_pos, mode='discriminator'):\n",
    "    fake = dis_convert(batch, fake_pos)\n",
    "    pred = discriminator(fake)\n",
    "    if mode == 'discriminator':\n",
    "        gt = get_gt(batch, fake_pos)\n",
    "        loss = criterion(pred, gt)\n",
    "    elif mode == 'generator':\n",
    "        losses = pred.sum(dim=0)\n",
    "#         losses = torch.tensor(config.importance).to(device) * losses #/ losses.detach()\n",
    "        loss = losses.sum()\n",
    "    else:\n",
    "        raise Exception\n",
    "    return -loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "3IE_84f-edNB"
   },
   "outputs": [],
   "source": [
    "def get_gan_loss(batch, fake_pos, mode='discriminator'):\n",
    "    return {\"sgan\": get_sgan_loss,\n",
    "            \"wgan\": get_wgan_loss,\n",
    "            \"rgan\": get_rgan_loss,\n",
    "            \"ragan\": get_ragan_loss,\n",
    "            \"dgdv2\": get_dgdv2_loss}[config.gan_flavor](batch, fake_pos, mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "id": "2NjekXgkedNB",
    "outputId": "d0ca2d2c-8af7-40af-b197-7974e566b064",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5545805a4f4b45f9a4e03fd9b5c56c6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(VBox(children=(Wrapper(), Hud())), HBox(children=(Output(),), layout=Layout(height='500px', over…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/slurmtmp.5513400/ipykernel_181719/3144612910.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malternate\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'epoch'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0mtrain_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;31m#     discriminator_scheduler.step()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/slurmtmp.5513400/ipykernel_181719/3144612910.py\u001b[0m in \u001b[0;36mtrain_gen\u001b[0;34m(batch, epoch)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m#train generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mgenerator_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mgenerator_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deepgd/lib/python3.9/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deepgd/lib/python3.9/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deepgd/lib/python3.9/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deepgd/lib/python3.9/site-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0mstate_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             F.adamw(params_with_grad,\n\u001b[0m\u001b[1;32m    111\u001b[0m                     \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deepgd/lib/python3.9/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# def train_dis(batch, epoch):\n",
    "#     generator.requires_grad_(False)\n",
    "#     discriminator.zero_grad()\n",
    "#     generator_output = generator(batch)\n",
    "#     if config.discriminator.noise.std > 0:\n",
    "#         generator_output = generator_output + torch.randn_like(generator_output) * config.discriminator.noise.std * config.discriminator.noise.decay ** epoch\n",
    "#     discriminator_loss = get_gan_loss(batch, generator_output, mode='discriminator')\n",
    "    \n",
    "#     # train discriminator\n",
    "#     discriminator_loss.backward()\n",
    "#     discriminator_optimizer.step()\n",
    "\n",
    "#     # gradient penalty\n",
    "#     if config.gp_weight > 0:\n",
    "#         discriminator.zero_grad()\n",
    "#         gp_loss = get_gp_loss(batch, generator_output, config.gp_weight)\n",
    "#         gp_loss.backward()\n",
    "#         discriminator_optimizer.step()\n",
    "\n",
    "#     hud['dis_loss'] = format(discriminator_loss.item(), '.2e')\n",
    "#     pbar().update()\n",
    "\n",
    "def train_gen(batch, epoch):\n",
    "    generator.requires_grad_(True)\n",
    "    generator.zero_grad()\n",
    "    discriminator.zero_grad()\n",
    "    generator_output = generator(batch)\n",
    "    if config.discriminator.noise.std > 0:\n",
    "        generator_output = generator_output + torch.randn_like(generator_output) * config.discriminator.noise.std * config.discriminator.noise.decay ** epoch\n",
    "    generator_loss = get_gan_loss(batch, generator_output, mode='generator') \n",
    "    \n",
    "    #train generator\n",
    "    generator_loss.backward()\n",
    "    generator_optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        dis_batch = dis_convert(batch, generator_output)\n",
    "        stress = stress_criterion(dis_batch).mean()\n",
    "        critic = discriminator(dis_batch).mean()\n",
    "    hud.append({'gen_loss': format(generator_loss.item(), '.2e'),\n",
    "                'stress': format(stress.item(), '.2e'),\n",
    "                'critic': format(critic.item(), '.2e')})\n",
    "    pbar().update()\n",
    "\n",
    "def cuda_memsafe_map(fn, *iterables, summary=False):\n",
    "    total, failed = 0, 0\n",
    "    iterator = zip(*iterables)\n",
    "    items = None\n",
    "    while True:\n",
    "        try:\n",
    "            items = next(iterator)\n",
    "            yield fn(*items)\n",
    "        except StopIteration:\n",
    "            if summary:\n",
    "                print(f'Iteration finished. {failed} out of {total} failed!')\n",
    "            break\n",
    "#         except RuntimeError:\n",
    "#             print('CUDA memory overflow! Skip batch...')\n",
    "#             del items\n",
    "#             failed += 1\n",
    "        torch.cuda.empty_cache()\n",
    "        total += 1\n",
    "    \n",
    "def validate(model, data_loader, criterion=val_criterion):\n",
    "    def val_one_batch(batch):\n",
    "        batch = preprocess_batch(model, batch)\n",
    "        pred = CanonicalizationByStress()(model(batch), batch)\n",
    "        gt = CanonicalizationByStress()(batch.gt_pos, batch)\n",
    "        loss = criterion(pred, batch)\n",
    "        gt_loss = criterion(gt, batch)\n",
    "        spc = (loss - gt_loss) / torch.maximum(torch.maximum(loss, gt_loss), torch.ones_like(loss)*1e-5)\n",
    "        return loss.mean().item(), spc.mean().item()\n",
    "    loss_all, spc_all = zip(*cuda_memsafe_map(val_one_batch, data_loader))\n",
    "    return np.mean(loss_all), np.mean(spc_all)\n",
    "\n",
    "def log(msg):\n",
    "    msg = f\"[{epoch:03}] {msg}\"\n",
    "    print(msg, file=open(paths.log(), \"a\"))\n",
    "    with log_out: \n",
    "        print(msg)\n",
    "\n",
    "print(f\"{'='*10} {config.link} {'='*10}\", file=open(paths.log(), \"a\"))\n",
    "hud = Hud()\n",
    "pbar = Wrapper(tqdm, total=len(train_loader), smoothing=0)\n",
    "plot_out = Output()\n",
    "log_out = Output()\n",
    "tabs = {\"status\": VBox([pbar, hud]), \n",
    "        \"plot\": HBox([plot_out], layout=Layout(height='500px', overflow_y='auto')),\n",
    "        \"log\": HBox([log_out], layout=Layout(height='500px', overflow_y='auto'))}\n",
    "tab_bar = Tab(children=list(tabs.values()))\n",
    "[tab_bar.set_title(i, name) for i, name in enumerate(tabs)]\n",
    "display(tab_bar)\n",
    "while True:\n",
    "    if epoch % config.log_interval == 0:\n",
    "        torch.save(generator.state_dict(), f\"{paths.checkpoints()}/gen_epoch_{epoch}.pt\")\n",
    "        torch.save(generator_optimizer.state_dict(), f\"{paths.checkpoints()}/gen_optim_epoch_{epoch}.pt\")\n",
    "#         torch.save(discriminator.state_dict(), f\"{paths.checkpoints()}/dis_epoch_{epoch}.pt\")\n",
    "#         torch.save(discriminator_optimizer.state_dict(), f\"{paths.checkpoints()}/dis_optim_epoch_{epoch}.pt\")\n",
    "        generator.eval()\n",
    "        with torch.no_grad():\n",
    "            val_stress, val_stress_spc = validate(model=generator, data_loader=val_loader)\n",
    "            val_xing, val_xing_spc = validate(model=generator, data_loader=val_loader, criterion=xing_criterion)\n",
    "            with plot_out:\n",
    "                fig = plt.figure()\n",
    "                graph_vis(G_list[11100], generator(make_batch(data_list[11100]).to(device)).cpu())\n",
    "                plt.show()\n",
    "#         for i, fn in enumerate(loss_fns):\n",
    "#             tensorboard.add_scalars(type(fn).__name__, {'train': train_loss_comp[i].item(), \n",
    "#                                                   'validation': val_loss_comp[i].item()}, epoch)\n",
    "        hud.append({\n",
    "            'val_stress': format(val_stress, '.2f'),\n",
    "            'val_stress_spc': format(val_stress_spc, '.2%'),\n",
    "            'val_xing': format(val_xing, '.2f'),\n",
    "            'val_xing_spc': format(val_xing_spc, '.2%'),\n",
    "        })\n",
    "        log(msg := f\"stress={hud.data['val_stress']}({hud.data['val_stress_spc']}) xing={hud.data['val_xing']}({hud.data['val_xing_spc']})\")\n",
    "    \n",
    "        tensorboard.add_scalars('Stress', {'Value': val_stress, 'SPC': val_stress_spc}, epoch)\n",
    "        tensorboard.add_scalars('Xing', {'Value': val_xing, 'SPC': val_xing_spc}, epoch)\n",
    "        tensorboard.add_figure('Rome/11100', fig, epoch)\n",
    "        tensorboard.add_text('Log', msg, epoch)\n",
    "        \n",
    "    # handle.update(tab_bar)\n",
    "    pbar().reset()\n",
    "    pbar().set_description(desc=f\"[epoch {epoch}/{config.epoch.end}]\")\n",
    "    hud(title=f\"epoch {epoch}\")\n",
    "    generator.train()\n",
    "#     discriminator.train()\n",
    "    # proper: proper layout\n",
    "#     for _ in range(config.discriminator.repeat):\n",
    "#         for batch in train_loader:\n",
    "#             train_dis(batch, epoch)\n",
    "#             if config.alternate == 'iteration':\n",
    "#                 train_gen(batch, epoch)\n",
    "\n",
    "    if config.alternate == 'epoch':\n",
    "        for batch in train_loader:\n",
    "            train_gen(batch, epoch)\n",
    "\n",
    "#     discriminator_scheduler.step()\n",
    "    generator_scheduler.step()\n",
    "\n",
    "    if epoch == config.epoch.end:\n",
    "        break\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.n = torch.tensor(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch.from_data_list([data]).to_data_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric as pyg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isinstance(pyg.data.Batch.from_data_list([pyg.data.Data()]), pyg.data.Batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pyg.data.Batch.from_data_list([pyg.data.Data()])) is pyg.data.Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_batch(generator, data_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0qYK_X_hrq9o"
   },
   "source": [
    "# Testdata_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XEvmAtNjWhcY"
   },
   "outputs": [],
   "source": [
    "test_epoch = -1\n",
    "\n",
    "test_generator = Generator(**config.generator.params[...]).to(device)\n",
    "test_ckpt_epoch = get_ckpt_epoch(paths.checkpoints(), test_epoch)\n",
    "test_ckpt_file = f\"{paths.checkpoints()}/gen_epoch_{test_ckpt_epoch}.pt\"\n",
    "print(f\"Loading from {test_ckpt_file}...\")\n",
    "test_generator.load_state_dict(torch.load(test_ckpt_file, map_location=torch.device(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZubmSTcYrq9o"
   },
   "outputs": [],
   "source": [
    "rotate = RotateByPrincipalComponents()\n",
    "def test_callback(*, idx, pred, metrics):\n",
    "    # graph_vis(G_list[idx], pred, file_name=f\"{paths.visualization()}/{idx}_{metrics['stress']:.2f}_{metrics['resolution_score']:.2f}.png\")\n",
    "    pred = rotate(torch.tensor(pred), data_list[idx])\n",
    "    graph_vis(G_list[idx], pred)\n",
    "    plt.title(f\"[pred] idx: {idx}, stress: {metrics['stress']:.2f}({metrics['stress_spc']:.2%}), xing: {metrics['xing']:.2f}({metrics['xing_spc']:.2%})\")\n",
    "    plt.show()\n",
    "    gt_pos = rotate(data_list[idx].gt_pos, data_list[idx])\n",
    "    graph_vis(G_list[idx], gt_pos, node_color='orange')\n",
    "    plt.title(f\"[gt] idx: {idx}, stress: {load_ground_truth(idx, 'stress')}, xing: {load_ground_truth(idx, 'xing')}\")\n",
    "    plt.show()\n",
    "    \n",
    "test_metrics = test(model=test_generator, \n",
    "                    criteria_list=[], \n",
    "                    dataset=data_list, \n",
    "                    idx_range=range(10000, 11000), \n",
    "                    callback=test_callback,\n",
    "                    gt_file='gt.csv')\n",
    "pickle.dump(test_metrics, open(paths.metrics(\"test\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t2rFXIr_PsrJ"
   },
   "outputs": [],
   "source": [
    "metrics = test_metrics\n",
    "print('stress:', metrics['stress'].mean())\n",
    "print('stress_spc:', metrics['stress_spc'].mean())\n",
    "print('xing:', metrics['xing'].mean())\n",
    "print('xing_spc:', metrics['xing_spc'].mean())\n",
    "print('l1_angle:', metrics['l1_angle'].mean())\n",
    "print('l1_angle_spc:', metrics['l1_angle_spc'].mean())\n",
    "print('edge:', metrics['edge'].mean())\n",
    "print('edge_spc:', metrics['edge_spc'].mean())\n",
    "print('ring:', metrics['ring'].mean())\n",
    "print('ring_spc:', metrics['ring_spc'].mean())\n",
    "print('tsne:', metrics['tsne'].mean())\n",
    "print('tsne_spc:', metrics['tsne_spc'].mean())\n",
    "print('reso_score:', metrics['resolution_score'].mean())\n",
    "print('min_angle:', metrics['min_angle'].mean())\n",
    "pd.DataFrame(map(lambda m: f\"{metrics[m].mean().item():.4f}\", list(metrics.keys())[:-1])).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L5S1HWnHOWIl"
   },
   "source": [
    "# Large Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_pefPcV1OZdD"
   },
   "outputs": [],
   "source": [
    "scalability = pd.read_csv(f\"/__artifacts__/data/scalability.csv\", index_col=\"index\")\n",
    "scalability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oMhWAsdvPnTX"
   },
   "outputs": [],
   "source": [
    "rescale = CanonicalizationByStress()\n",
    "stressfn = Stress()\n",
    "rotate = RotateByPrincipalComponents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LR83OcAaPID5"
   },
   "outputs": [],
   "source": [
    "stress_list = []\n",
    "spc_list = []\n",
    "pmds_list = np.load(\"layouts/new_large_graph/pmds.npy\", allow_pickle=True)\n",
    "gviz_list = np.load(\"layouts/new_large_graph/gviz.npy\", allow_pickle=True)\n",
    "with torch.no_grad():\n",
    "    for idx, col in tqdm(scalability.iterrows(), total=len(scalability)):\n",
    "        # if idx not in [406, 516]: continue\n",
    "        torch.cuda.empty_cache()\n",
    "        G = load_mtx(col['file'])\n",
    "        G.remove_edges_from(nx.selfloop_edges(G))\n",
    "        data = generate_data_list(G, \n",
    "                                sparse=data_config.sparse, \n",
    "                                pivot_mode=data_config.pivot,\n",
    "                                init_mode=data_config.init,\n",
    "                                edge_index=data_config.edge.index,\n",
    "                                edge_attr=data_config.edge.attr,\n",
    "                                pmds_list=pmds_list[idx],\n",
    "                                gviz_list=gviz_list[idx],\n",
    "                                device=device)\n",
    "        batch = Batch.from_data_list([data]).to(device)\n",
    "        # generator.train()\n",
    "        # generator(batch)\n",
    "        generator.eval()\n",
    "        pred = generator(batch)\n",
    "        pos = rotate(rescale(pred, batch), batch)\n",
    "        gt = rotate(rescale(batch.gt_pos, batch), batch)\n",
    "        stress = stressfn(pos, batch).item()\n",
    "        gt_stress = stressfn(gt, batch).item()\n",
    "        spc = (stress - gt_stress) / np.maximum(stress, gt_stress)\n",
    "        stress_list.append(stress)\n",
    "        spc_list.append(spc)\n",
    "\n",
    "        np.save(f\"/__artifacts__/gan_result/data/scalability_{idx}.npy\", pos.cpu().numpy())\n",
    "        graph_attr = dict(node_size=1, \n",
    "                        with_labels=False, \n",
    "                        labels=dict(zip(list(G.nodes), map(lambda n: n if type(n) is int else n[1:], list(G.nodes)))),\n",
    "                        font_color=\"white\", \n",
    "                        font_weight=\"bold\",\n",
    "                        font_size=12,\n",
    "                        width=0.1)\n",
    "\n",
    "        # gt_pos = pickle.load(open(f\"/__artifacts__/data/scalability_{idx}_gt.pkl\", \"rb\"))\n",
    "\n",
    "        plt.figure(figsize=[12, 9])\n",
    "        nx.draw(G, pos=gt.cpu().numpy(), node_color='orange', **graph_attr)\n",
    "        plt.title(f\"neato: large_{idx}\")\n",
    "        plt.axis(\"equal\")\n",
    "        plt.savefig(f\"/__artifacts__/gan_result/output/{idx}_{col['name']}_{col['n']}_{spc}_nx.png\", dpi=300)\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=[12, 9])\n",
    "        graph_vis(G, pos.cpu().numpy(), **graph_attr)\n",
    "        plt.title(f\"dgd: large_{idx}, spc={spc:.2%}\")\n",
    "        plt.axis(\"equal\")\n",
    "        plt.savefig(f\"/__artifacts__/gan_result/output/{idx}_{col['name']}_{col['n']}_{spc}_dgd.png\", dpi=300)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gRXTewkYeuq1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "w_p3OD0woiSM",
    "DR6-vYtr_i_P",
    "innqkwvH_ydD",
    "f-ODE2k8BFV6"
   ],
   "machine_shape": "hm",
   "name": "GAN(gan=rgan,data=best(xing,stress),dis=deep,share=10,embed=6,gp=0).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "DeepGD",
   "language": "python",
   "name": "deepgd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
