{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config('config-Copy11.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'noinput_repoch_dropout',\n",
       " 'batchsize': 8,\n",
       " 'epoch': {'start': 1, 'end': None},\n",
       " 'lr': {'initial': 0.001,\n",
       "  'decay_rate': 0.99,\n",
       "  'decay_step': 1,\n",
       "  'override': None},\n",
       " 'log_period': 1,\n",
       " 'test': {'name': 'noinput_repoch_dropout',\n",
       "  'epoch': 431,\n",
       "  'out_prefix': 'drop_sd'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config[...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Net_NOD_BN_Skip_SuperDeep(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        edge_feats = 2\n",
    "        \n",
    "        self.p = nn.Parameter(torch.zeros(16))\n",
    "        self.conv1 = NNConv(16, 32, Linear(edge_feats, 16*32), aggr='mean')\n",
    "        self.conv2 = NNConv(32, 64, Linear(edge_feats, 32*64), aggr='mean')\n",
    "        self.conv3 = NNConv(64, 64, Linear(edge_feats, 64*64), aggr='mean')\n",
    "        self.conv4 = NNConv(64, 64, Linear(edge_feats, 64*64), aggr='mean')\n",
    "        self.conv5 = NNConv(64, 64, Linear(edge_feats, 64*64), aggr='mean')\n",
    "        self.conv6 = NNConv(64, 64, Linear(edge_feats, 64*64), aggr='mean')\n",
    "        self.conv7 = NNConv(64, 64, Linear(edge_feats, 64*64), aggr='mean')\n",
    "        self.conv8 = NNConv(64, 64, Linear(edge_feats, 64*64), aggr='mean')\n",
    "        self.conv9 = NNConv(64, 64, Linear(edge_feats, 64*64), aggr='mean')\n",
    "        self.conv10 = NNConv(64, 64, Linear(edge_feats, 64*64), aggr='mean')\n",
    "        self.conv11 = NNConv(64, 64, Linear(edge_feats, 64*64), aggr='mean')\n",
    "        self.conv12 = NNConv(64, 64, Linear(edge_feats, 64*64), aggr='mean')\n",
    "        self.conv13 = NNConv(64, 32, Linear(edge_feats, 32*64), aggr='mean')\n",
    "        self.conv14 = NNConv(32, 2, Linear(edge_feats, 2*32), aggr='mean')\n",
    "\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.bn1 = BatchNorm(32)\n",
    "        self.bn2 = BatchNorm(64)\n",
    "        self.bn3 = BatchNorm(64)\n",
    "        self.bn4 = BatchNorm(64)\n",
    "        self.bn5 = BatchNorm(64)\n",
    "        self.bn6 = BatchNorm(64)\n",
    "        self.bn7 = BatchNorm(64)\n",
    "        self.bn8 = BatchNorm(64)\n",
    "        self.bn9 = BatchNorm(64)\n",
    "        self.bn10 = BatchNorm(64)\n",
    "        self.bn11 = BatchNorm(64)\n",
    "        self.bn12 = BatchNorm(64)\n",
    "        self.bn13 = BatchNorm(32)\n",
    "        \n",
    "        self.dp1 = Dropout(0.2)\n",
    "        self.dp2 = Dropout(0.2)\n",
    "        self.dp3 = Dropout(0.2)\n",
    "        self.dp4 = Dropout(0.2)\n",
    "        self.dp5 = Dropout(0.2)\n",
    "        self.dp6 = Dropout(0.2)\n",
    "        self.dp7 = Dropout(0.2)\n",
    "        self.dp8 = Dropout(0.2)\n",
    "        self.dp9 = Dropout(0.2)\n",
    "        self.dp10 = Dropout(0.2)\n",
    "        self.dp11 = Dropout(0.2)\n",
    "        self.dp12 = Dropout(0.2)\n",
    "\n",
    "\n",
    "\n",
    "#         self.conv4 = NNConv(16+32+64, 128, Linear(2, (16+32+64)*128))\n",
    "#         self.conv5 = NNConv(128,2,Linear(2,128*2))\n",
    "#         self.conv2 = NNConv(16,2,Linear(2,16*2))\n",
    "\n",
    "    def forward(self, data):\n",
    "        edge_index, edge_attr = data.edge_index, data.edge_attr\n",
    "\n",
    "        x = self.p.repeat(len(data.x), 1)\n",
    "        x1 = self.dp1(self.relu(self.bn1(self.conv1(x, edge_index, edge_attr))))\n",
    "        x2 = self.dp2(self.relu(self.bn2(self.conv2(x1, edge_index, edge_attr))))\n",
    "        x3 = self.dp3(self.relu(self.bn3(self.conv3(x2, edge_index, edge_attr))))\n",
    "        x4 = self.dp4(self.relu(self.bn4(self.conv4(x3, edge_index, edge_attr))))\n",
    "        \n",
    "        res_x4 = x2+x4\n",
    "        \n",
    "        x5 = self.dp5(self.relu(self.bn5(self.conv5(res_x4, edge_index, edge_attr))))\n",
    "        x6 = self.dp6(self.relu(self.bn6(self.conv6(x5, edge_index, edge_attr))))\n",
    "                            \n",
    "        res_x6 = res_x4+x6\n",
    "                         \n",
    "        x7 = self.dp7(self.relu(self.bn7(self.conv7(res_x6, edge_index, edge_attr))))\n",
    "        x8 = self.dp8(self.relu(self.bn8(self.conv8(x7, edge_index, edge_attr))))\n",
    "                            \n",
    "        res_x8 = res_x6+x8\n",
    "                         \n",
    "        x9 = self.dp9(self.relu(self.bn9(self.conv9(res_x8, edge_index, edge_attr))))\n",
    "        x10 = self.dp10(self.relu(self.bn10(self.conv10(x9, edge_index, edge_attr))))\n",
    "                            \n",
    "        res_x10 = res_x8+x10\n",
    "                         \n",
    "        x11 = self.dp11(self.relu(self.bn11(self.conv11(res_x10, edge_index, edge_attr))))\n",
    "        x12 = self.dp12(self.relu(self.bn12(self.conv12(x11, edge_index, edge_attr))))\n",
    "                            \n",
    "        res_x12 = res_x10+x12\n",
    "                                        \n",
    "        x13 = self.relu(self.bn13(self.conv13(res_x12, edge_index, edge_attr)))\n",
    "        x14 = self.conv14(x13, edge_index, edge_attr)\n",
    "\n",
    "        return x14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_processed_data():\n",
    "    if os.path.isfile('G_list.pickle') and os.path.isfile('data_list.pickle'):\n",
    "        G_list = pickle.load(open('G_list.pickle', 'rb'))\n",
    "        data_list = pickle.load(open('data_list.pickle', 'rb'))\n",
    "    else:\n",
    "        if not os.path.isfile('data_index.txt'):\n",
    "            shuffle_rome('data_index.txt')\n",
    "        rome = load_rome('data_index.txt')\n",
    "        G_list, data_list = convert_datalist(rome)\n",
    "        pickle.dump(G_list, open('G_list.pickle', 'wb'))\n",
    "        pickle.dump(data_list, open('data_list.pickle', 'wb'))\n",
    "    return G_list, data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_list, data_list = load_processed_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(data_list[:10000], batch_size=config['batchsize'],shuffle=True)\n",
    "loss_ep = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_epoch = config['epoch']['start']\n",
    "if start_epoch == 1:\n",
    "    model = Net_NOD_BN_Skip_SuperDeep().to(device)\n",
    "else:\n",
    "    model = torch.load(f\"../ckpt_{config['name']}/epoch_{start_epoch}.pt\").to(device)\n",
    "criterion = EnergyLossVectorized()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr']['initial'])\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optimizer, \n",
    "                                            step_size=config['lr']['decay_step'], \n",
    "                                            gamma=config['lr']['decay_rate'])\n",
    "print(\"=\" * 50, file=open(f\"{config['name']}.log\", \"a\"))\n",
    "epoch = start_epoch\n",
    "with tqdm(total=len(loader), smoothing=0) as progress:\n",
    "    while True:\n",
    "        if config['lr']['override'] is not None:\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr']['override'])\n",
    "            scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optimizer, \n",
    "                                                        step_size=config['lr']['decay_step'], \n",
    "                                                        gamma=config['lr']['decay_rate'])\n",
    "        progress.reset()\n",
    "        progress.set_description(desc=f\"[epoch {epoch}/{config['epoch']['end']}]\")\n",
    "        train_loss = train(model, criterion, optimizer, loader, data_list, device, progress)\n",
    "        loss_ep.append(train_loss)\n",
    "        scheduler.step()\n",
    "        if epoch == 1 and config['log_period'] != 1:\n",
    "            print(epoch, loss, scheduler.get_lr(), file=open(f\"{config['name']}.log\", \"a\"))\n",
    "        if epoch % config['log_period'] == 0:\n",
    "            torch.save(model, f\"../ckpt_{config['name']}/epoch_{epoch}.pt\")\n",
    "            test_loss = []\n",
    "            for val_idx in range(11000, len(G_list)):\n",
    "                node_pos,loss = evaluate(model, data_list[val_idx],criterion,device)\n",
    "                test_loss.append(loss)\n",
    "            print(f'{epoch}, train: {train_loss}, val:{np.mean(test_loss)}，{scheduler.get_lr()}', file=open(f\"{config['name']}.log\", \"a\"))\n",
    "        if epoch == config['epoch']['end']:\n",
    "            break\n",
    "        for idx in range(10000):\n",
    "            data_list[idx].x = generate_randPos(data_list[idx].x.shape[0], random.randint(0, 100000000))\n",
    "        epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(f'../ckpt_{config[\"test\"][\"name\"]}/epoch_{config[\"test\"][\"epoch\"]}.pt', map_location=torch.device(device))\n",
    "criterion = EnergyLossVectorized()\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = pd.read_csv('ground_truth_loss.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c43a3daf86c24ca0939021b63f21492e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2531.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "folder_name = f'{config[\"test\"][\"name\"]}_test'\n",
    "for test_idx in tqdm(list(chain(range(0, 10000, 10), range(10000, len(data_list))))):\n",
    "    G_vis = G_list[test_idx]\n",
    "    node_pos,loss = evaluate(model, data_list[test_idx],criterion,device)\n",
    "    if 10000 <= test_idx < 11000:\n",
    "        gt_loss = ground_truth.loc[test_idx][0]\n",
    "        loss_ratio = (loss - gt_loss) / gt_loss\n",
    "    else:\n",
    "        loss_ratio = '?'\n",
    "    graph_vis(G_vis, node_pos, f'{folder_name}/{config[\"test\"][\"out_prefix\"]}_model_{test_idx}_{loss}_{loss_ratio}.png') \n",
    "    node_pos = nx.nx_agraph.graphviz_layout(G_vis, prog='neato')\n",
    "    plt.figure()\n",
    "    nx.draw(G_vis, node_pos)\n",
    "    plt.savefig(f'{folder_name}/{test_idx}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bcb9cedcbbf40e99c042d89837e143c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "loss_ratios = []\n",
    "for test_idx in tqdm(range(10000, 11000)):\n",
    "    G_vis = G_list[test_idx]\n",
    "    node_pos, loss = evaluate(model, data_list[test_idx], criterion, device)\n",
    "    gt_loss = ground_truth.loc[test_idx][0]\n",
    "    loss_ratio = (loss - gt_loss) / gt_loss\n",
    "    losses += [loss]\n",
    "    loss_ratios += [loss_ratio]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(node_pos),np.std(node_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(data_list[11000].x.numpy()),np.std(data_list[11000].x.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.9666, 4.760008302513768)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(losses), np.std(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.8557804214093896, 0.18999037869661115)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(loss_ratios), np.std(loss_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnergyLossScaled(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, p, data, scale):\n",
    "        edge_attr = data.edge_attr\n",
    "        # convert per-node positions to per-edge positions\n",
    "        start, end, n_nodes = node2edge(p, data)\n",
    "        \n",
    "        start *= scale\n",
    "        end *= scale\n",
    "        \n",
    "        start_x = start[:, 0]\n",
    "        start_y = start[:, 1]\n",
    "        end_x = end[:, 0]\n",
    "        end_y = end[:, 1]\n",
    "        \n",
    "        l = edge_attr[:, 0]\n",
    "        k = edge_attr[:, 1]\n",
    "        \n",
    "        term1 = (start_x - end_x) ** 2\n",
    "        term2 = (start_y - end_y) ** 2\n",
    "        term3 = l ** 2\n",
    "        term4 = 2 * l * (term1 + term2).sqrt()\n",
    "        energy = k / 2 * (term1 + term2 + term3 - term4)\n",
    "        return energy.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_scaled = EnergyLossScaled()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_loss = 0\n",
    "pred_loss = 0\n",
    "for idx in tqdm(range(10000, 11000)):\n",
    "    pred, loss = evaluate(model, data_list[idx], criterion, device)\n",
    "    pos_map = nx.nx_agraph.graphviz_layout(G_list[idx], prog='neato')\n",
    "\n",
    "    pred_mean, pred_std = pred.mean(axis=0), pred.std()\n",
    "    truth = np.array(list(pos_map.values()))\n",
    "    truth_mean, truth_std = truth.mean(axis=0), truth.std()\n",
    "    norm_truth = (truth - truth_mean) / truth_std\n",
    "    scaled_truth = norm_truth * pred_std + pred_mean\n",
    "\n",
    "    truth_loss += criterion(torch.tensor(scaled_truth), data_list[idx])\n",
    "    pred_loss += criterion(torch.tensor(pred), data_list[idx])\n",
    "    \n",
    "truth_loss / 1000, pred_loss / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, G = data_list[10000], G_list[10000]\n",
    "edge_attr = data.edge_attr\n",
    "pred, loss = evaluate(model, data, criterion, device)\n",
    "pos_map = nx.nx_agraph.graphviz_layout(G, prog='neato')\n",
    "truth = np.array(list(pos_map.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "start, end, n_nodes = node2edge(torch.tensor(truth, device=device), data)\n",
    "w = edge_attr[:, 1]\n",
    "d = edge_attr[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "u2 = ((start - end) ** 2).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = (w * d).sum() / (w * u2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(72.3337, device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion_scaled(torch.tensor(truth, device=device), data, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.79"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data_list[9999].x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 5\n",
    "losses = []\n",
    "folder_name = f'{config[\"test\"][\"name\"]}_iterative_test'\n",
    "for test_idx in tqdm(range(10000, len(G_list))):\n",
    "    G_vis = G_list[test_idx]\n",
    "    node_pos = nx.nx_agraph.graphviz_layout(G_vis, prog='neato')\n",
    "    plt.figure()\n",
    "    nx.draw(G_vis, node_pos)\n",
    "    plt.savefig(f'{folder_name}/{test_idx}.png')\n",
    "    for i in range(iterations):\n",
    "        node_pos, loss = evaluate(model, data_list[test_idx], criterion, device) \n",
    "        data_list[test_idx].x = torch.tensor(node_pos,dtype=torch.float)\n",
    "    losses += [loss]\n",
    "    graph_vis(G_vis, node_pos, f'{folder_name}/{config[\"test\"][\"out_prefix\"]}_iter_model_{test_idx}_{loss}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(loss_ep[:1000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
